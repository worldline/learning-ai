import{by as a,bz as n,bA as s,bF as t}from"./app-DmbIGF3-.js";const o="/learning-ai/assets/librechat_interface-BcD_tGyb.png",i="/learning-ai/assets/prompt-D9YI1LJh.png",r="/learning-ai/assets/demo_prompt-BpZVtafc.gif",l="/learning-ai/assets/assistant-BJUR0Mbm.png",c="/learning-ai/assets/tool_mixing-DboomOxj.png",p="/learning-ai/assets/lmstudio-B6PjFtYg.png",d={};function h(u,e){return t(),n("div",null,[...e[0]||(e[0]=[s('<h1 id="online-offline-llms-clients" tabindex="-1"><a class="header-anchor" href="#online-offline-llms-clients"><span>Online/Offline LLMs clients</span></a></h1><p>This section is dedicated to the usage of Generative AI models through different clients. We will explore both online and offline clients that allow you to interact with LLMs for various use cases. A client is a Graphical User Interface (GUI) that allows users to interact with LLMs without the need for coding. Clients provide a user-friendly interface for users to input prompts, configure settings, and view responses from the models. It can also provide additional features such as prompt management, conversation history, and integration with other tools and services.</p><p>A client can be :</p><ul><li>Web-based : ChatGPT, Bard, Claude, Gemini</li><li>Self-hosted : Librechat self-hosted instance</li><li>Desktop app : Claude desktop,</li><li>IDE plugin : ChatGPT for VS code</li><li>Mobile application : copilot for mobile, le chat ai mobile app</li></ul><p>Creators of LLMs often provide official clients for their models, but there are also third-party clients available that support multiple LLMs and offer additional features.</p><h2 id="online-librechat" tabindex="-1"><a class="header-anchor" href="#online-librechat"><span>Online (Librechat)</span></a></h2><p><img src="'+o+`" alt="librechat_interface"></p><p>LibreChat is a free, open source AI chat platform. This Web UI offers vast customization, supporting numerous AI providers, services, and integrations. Serves all AI Conversations in one place with a familiar interface, innovative enhancements, for as many users as you need.</p><p>The full librechat documentation is available <a href="https://www.librechat.ai/docs" target="_blank" rel="noopener noreferrer">here</a></p><p>Let&#39;s discover how to use LibreChat to create efficient and effective conversations with AI for developers.</p><h3 id="history" tabindex="-1"><a class="header-anchor" href="#history"><span>History</span></a></h3><p>Prompts history allows users to save and load prompts for their conversations and easily access them later. Reusing prompts can save time and effort, especially when working with multiple conversations and keep track of the context and details of a conversation.</p><p>Remember to :</p><ul><li>switch to a new conversation when a new topic is started</li><li>take an older conversation when you want to continue a previous topic</li></ul><h3 id="bookmarks" tabindex="-1"><a class="header-anchor" href="#bookmarks"><span>Bookmarks</span></a></h3><p>Keep the conversation organized by bookmarking them. You can easily find the conversations you want to access later with bookmarks. A bookmark is a group of conversations grouped with a tag name</p><h3 id="custom-instructions" tabindex="-1"><a class="header-anchor" href="#custom-instructions"><span>Custom Instructions</span></a></h3><p>Custom instructions allow users to set specific guidelines for the AI to follow during conversations. This feature helps tailor the AI&#39;s responses to better suit the user&#39;s needs and preferences.</p><h3 id="memories" tabindex="-1"><a class="header-anchor" href="#memories"><span>Memories</span></a></h3><p>Memories allow users to save important information or context from previous conversations to be used in future interactions. This feature helps maintain continuity and relevance in conversations by providing the AI with relevant background information.</p><h3 id="prompts" tabindex="-1"><a class="header-anchor" href="#prompts"><span>Prompts</span></a></h3><p>The prompts feature allows users to save and load predefined prompts to use it during their conversations. You can use a prompt with the /[<code>prompt command</code>]. A prompt can have parameters, which are replaced with values when the prompt is used.</p><p><strong>Exemple</strong></p><ul><li>On the right side menu, go to the <code>ðŸ’¬ Prompts</code> tab.</li><li>Click on the <code>+ add a new prompt</code>.</li><li>name your prompt : <code>explain</code></li><li>on Text tab, you can write your prompt :</li></ul><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code class="language-text"><span class="line">Explain the following {{language:Java|Kotlin|Javascript}} snippet of code:</span>
<span class="line">{{code}}</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="`+i+'" alt="preformatted_prompts_config"></p><ul><li>Now you can use the <code>/explain</code> command to get the explanation of the code snippet.</li></ul><p><img src="'+r+'" alt="preformatted_prompts_usage"></p><h3 id="agent-builder" tabindex="-1"><a class="header-anchor" href="#agent-builder"><span>Agent Builder</span></a></h3><p>Librechat Agent Builder allows users to create custom AI agents that can perform specific tasks or functions. Users can define the agent&#39;s behavior, capabilities, and interactions with users. This feature enables users to build specialized AI assistants tailored to their needs. The agent can be shared on a marketplace for other users to use it.</p><h3 id="models" tabindex="-1"><a class="header-anchor" href="#models"><span>Models</span></a></h3><h4 id="default-model" tabindex="-1"><a class="header-anchor" href="#default-model"><span>Default model</span></a></h4><p>By default, LibreChat uses the OpenAI GPT- nano model to generate responses. This model is a smaller version of the GPT-3 model, which is designed to be more efficient and faster while still providing high-quality responses and also be cost effective.</p><h4 id="worldline-agent" tabindex="-1"><a class="header-anchor" href="#worldline-agent"><span>Worldline Agent</span></a></h4><p>The Worldline Agent is a custom AI agent built using LibreChat&#39;s Agent Builder. It is designed to assist users with tasks related to Worldline&#39;s services and products. The Worldline Agent can provide information, answer questions, and perform specific actions based on user requests.</p><h4 id="reasoning-model-for-code-creativity" tabindex="-1"><a class="header-anchor" href="#reasoning-model-for-code-creativity"><span>Reasoning model for code/creativity</span></a></h4><p>For tasks that require reasoning, such as code generation or creative writing, it is recommended to use a more powerful model like Claude Haiku. These models are better suited for complex tasks that require a deeper understanding of context and logic.</p><h4 id="long-context-model" tabindex="-1"><a class="header-anchor" href="#long-context-model"><span>Long context model</span></a></h4><p>For tasks that require a long context, such as document summarization or long conversations, it is recommended to use a model that supports long context, like Gemini Pro. These models can handle larger amounts of text and maintain context over longer interactions.</p><h4 id="translation-model" tabindex="-1"><a class="header-anchor" href="#translation-model"><span>Translation model</span></a></h4><p>For translation tasks, it is recommended to use specialized translation model</p><h4 id="azure-openai" tabindex="-1"><a class="header-anchor" href="#azure-openai"><span>Azure OpenAI</span></a></h4><p>Azure OpenAI provides access to OpenAI&#39;s powerful language models through Microsoft&#39;s Azure cloud platform. It allows developers to integrate advanced AI capabilities into their applications, such as natural language understanding, text generation, and more.</p><h4 id="google-gemini" tabindex="-1"><a class="header-anchor" href="#google-gemini"><span>Google Gemini</span></a></h4><p>Gemini is a large language model (LLM) developed by Google. It&#39;s designed to be a multimodal AI, meaning it can work with and understand different types of information, including text, code, audio, and images. Google positions Gemini as a highly capable model for a range of tasks, from answering questions and generating creative content to problem-solving and more complex reasoning. There are different versions of Gemini, optimized for different tasks and scales.</p><h4 id="anthropic-claude" tabindex="-1"><a class="header-anchor" href="#anthropic-claude"><span>Anthropic Claude</span></a></h4><p>Claude is an Artificial Intelligence, trained by Anthropic. Claude can process large amounts of information, brainstorm ideas, generate text and code, help you understand subjects, coach you through difficult situations, help simplify your busywork so you can focus on what matters most, and so much more.</p><h3 id="assistants" tabindex="-1"><a class="header-anchor" href="#assistants"><span>Assistants</span></a></h3><h4 id="creating-assistants" tabindex="-1"><a class="header-anchor" href="#creating-assistants"><span>Creating Assistants</span></a></h4><p>The Assistants API enables the creation of AI assistants, offering functionalities like code interpreter, knowledge retrieval of files, and function execution. The Assistants API allows you to build AI assistants within your own applications for specific needs. An Assistant has instructions and can leverage models, tools, and files to respond to user queries. The Assistants API currently supports three types of tools: Code Interpreter, File Search, and Function calling.</p><p><img src="'+l+'" alt="assistant"></p><h4 id="marketplace" tabindex="-1"><a class="header-anchor" href="#marketplace"><span>Marketplace</span></a></h4><p>The Marketplace is a collection of pre-built AI assistants that users can browse, install, and use within LibreChat. These assistants are designed to perform specific tasks or provide specialized knowledge in various domains. Users can explore the marketplace to find assistants that suit their needs and enhance their interactions with AI.</p><p>You can access the marketplace from the top left side menu by clicking on the <code>Marketplace</code> icon.</p><div class="hint-container tip"><p class="hint-container-title">Image generation assistants</p><p>There are image generation assistants available in the marketplace that use models such as DALL-E</p></div><h3 id="tools" tabindex="-1"><a class="header-anchor" href="#tools"><span>Tools</span></a></h3><p>Tools section provide standalone tools that can be used to enhance your conversations with LLMs. These tools can be used to add additional knowledge and functionalities to your conversations.</p><p>The plugins endpoint opens the door to prompting LLMs in new ways other than traditional input/output prompting.</p><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>Every additional plugin selected will increase your token usage as there are detailed instructions the LLM needs for each one For best use, be selective with plugins per message and narrow your requests as much as possible</p></div><h4 id="web-search" tabindex="-1"><a class="header-anchor" href="#web-search"><span>Web search</span></a></h4><p>Retrieve data from internet and use it to generate a response.</p><h4 id="file-search" tabindex="-1"><a class="header-anchor" href="#file-search"><span>File search</span></a></h4><p>Upload files to the conversation and use them to generate a response.</p><h4 id="artifacts" tabindex="-1"><a class="header-anchor" href="#artifacts"><span>Artifacts</span></a></h4><p>Add complementary knowledge to your conversations with Artifacts such as visual preview of html pages.</p><h3 id="mcp-servers" tabindex="-1"><a class="header-anchor" href="#mcp-servers"><span>MCP servers</span></a></h3><p>MCP section provide remote tools to add complementary knowledge and functionalities to your LLM conversations. On librechat a list of MCP servers is available to connect to your conversations provided by your organization or third parties such as slack, chart, confluence, jira</p><h3 id="tools-mixing" tabindex="-1"><a class="header-anchor" href="#tools-mixing"><span>Tools mixing</span></a></h3><p>You can mix too to create more complex prompts. For example, you can retrieve data from internet with the Browser tool and use the chart tool to create a graph based on the retrieved data.</p><p><img src="'+c+'" alt="tool_mixing"></p><h3 id="rag" tabindex="-1"><a class="header-anchor" href="#rag"><span>RAG</span></a></h3><p>RAG is possible with LibreChat. You can use RAG to create a conversation with the AI. To can add files to the conversation, you go to the file tab and select the file you want to add. Then the file will be added to the file manager and you can use it in the prompt.</p><p>The file can be an png, a video, a text file, or a PDF file.</p><h3 id="ðŸ§ª-exercises" tabindex="-1"><a class="header-anchor" href="#ðŸ§ª-exercises"><span>ðŸ§ª Exercises</span></a></h3><h4 id="_1-prompt-creation" tabindex="-1"><a class="header-anchor" href="#_1-prompt-creation"><span>1. Prompt creation</span></a></h4><p>Select one prompt engineering technique and make a prompt in librechat that can be called with the <code>/[prompt_name]</code> command.</p><h4 id="_2-plugins-mixing" tabindex="-1"><a class="header-anchor" href="#_2-plugins-mixing"><span>2. Plugins mixing</span></a></h4><p>Use the Browse and Chart tool to create a prompt that generates a chart from data retrieved from worldline website https://worldline.com/en/home/top-navigation/about-worldline/who-we-are</p><h4 id="_3-make-your-own-assistant" tabindex="-1"><a class="header-anchor" href="#_3-make-your-own-assistant"><span>3. Make your own assistant</span></a></h4><p>Choose your favorite topic ( cooking, travel, sports, etc.) and create an assistant that can answer questions about it. You can share documents, files and instructions to configure your custom assistant and use it.</p><h2 id="offline-lm-studio" tabindex="-1"><a class="header-anchor" href="#offline-lm-studio"><span>Offline (LM Studio)</span></a></h2><div class="hint-container warning"><p class="hint-container-title">Disclamer</p><p>Be careful with offline prompting models downloaded from the internet. They can contain malicious code. And also the size of the model can be very large from few Gb to few Tb.</p></div><h3 id="definitions" tabindex="-1"><a class="header-anchor" href="#definitions"><span>Definitions</span></a></h3><p>If you don&#39;t want to use the online AI providers, you can use offline prompting. This technique involves using a local LLM to generate responses to prompts. It is useful for developers who want to use a local LLM for offline prompting or for those who want to experiment with different LLMs without relying on online providers.</p><p>LM Studio is a tool that allows developers to experiment with different LLMs without relying on online providers. It provides a user-friendly interface for selecting and configuring LLMs, as well as a chat interface for interacting with the LLMs. It also includes features for fine-tuning and deploying LLMs. This technique is useful for developers who want to experiment with different LLMs.</p><h3 id="installation" tabindex="-1"><a class="header-anchor" href="#installation"><span>Installation</span></a></h3><p><img src="'+p+`" alt="lmstudio_installation"></p><p>For installation, you can follow the instructions <a href="https://lmstudio.ai/docs/" target="_blank" rel="noopener noreferrer">here</a></p><h3 id="model-configuration" tabindex="-1"><a class="header-anchor" href="#model-configuration"><span>Model configuration</span></a></h3><p>You can configure the model you want to use in the settings tab. You can select the model you want to use and configure it according to your needs.</p><p><code>Context Length</code>: The context length is the number of tokens that will be used as context for the model. This is important because it determines how much information the model can use to generate a response. A longer context length will allow the model to generate more detailed and relevant responses, but it may also increase the computational cost of the model.</p><p><code>GPU Offload</code>: This option allows you to offload the model to a GPU if available. This can significantly speed up the generation process, especially for longer prompts or complex models.</p><p><code>CPU Threads</code>: This option allows you to specify the number of CPU threads to use for the model. This can be useful for controlling the computational resources used by the model.</p><p><code>Evaluation batch size</code>: This option allows you to specify the batch size for evaluation. This is important for evaluating the performance of the model and can affect the speed and accuracy of the generation process.</p><p><a href="https://www.hopsworks.ai/dictionary/rope-scaling" target="_blank" rel="noopener noreferrer"><code>RoPE Frequency base</code></a>: The Rotary Position Embeddings is a technique used to improve the performance of transformer-based models. This is important for controlling the output length of the model and can affect the quality of the generated responses.</p><p><code>Keep model in memory</code>: This option allows you to keep the model in memory after the generation process is complete. This can be useful for generating multiple responses or for using the model for offline prompting.</p><p><code>Try mmap()</code> for faster loading: This option allows you to try using mmap() for faster loading of the model. This can be useful for loading large models or for generating responses quickly.</p><p><code>Seed</code>: This option allows you to specify a seed for the model. This can be useful for controlling the randomness of the generated responses.</p><p><code>Flash Attention</code>: This option allows you to enable flash attention for the model. This can be useful for generating more detailed and accurate responses, but it may also increase the computational cost of the model.</p><h3 id="enable-apis" tabindex="-1"><a class="header-anchor" href="#enable-apis"><span>enable APIs</span></a></h3><p>You can use the APIs to generate responses from the models. To enable the API server with LM Studio, you need to set the <code>API Server</code> option to <code>ON</code> in the settings tab. You can then use the API endpoints to generate responses from the models.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Success<span class="token operator">!</span> HTTP server listening on port <span class="token number">1234</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Supported endpoints:</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	GET  http://localhost:1234/v1/models</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/chat/completions</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/embeddings</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Logs are saved into /Users/ibrahim/.cache/lm-studio/server-logs</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> Server started.</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> Just-in-time model loading active.</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You can use the endpoints to generate responses from the models. The endpoints are as follows:</p><ul><li><code>GET /v1/models</code>: This endpoint returns a list of the available models.</li><li><code>POST /v1/chat/completions</code>: This endpoint generates responses from the models using the chat format.Chat format is used for tasks such as chatbots, conversational AI, and language learning.</li><li><code>POST /v1/embeddings</code>: This endpoint generates embeddings from the models. Embeddings are used for tasks such as sentiment analysis, text classification, and language translation.</li></ul><h3 id="demo" tabindex="-1"><a class="header-anchor" href="#demo"><span>DEMO</span></a></h3><h3 id="ðŸ§ª-exercises-1" tabindex="-1"><a class="header-anchor" href="#ðŸ§ª-exercises-1"><span>ðŸ§ª Exercises</span></a></h3><ol><li>Install LM Studio and configure the model you want to use.</li><li>Enable the API server and test the endpoints using curl or Postman.</li></ol><h2 id="ðŸ“–-further-readings" tabindex="-1"><a class="header-anchor" href="#ðŸ“–-further-readings"><span>ðŸ“– Further readings</span></a></h2><ul><li><a href="https://worldline365.sharepoint.com/:u:/r/sites/GenerativeAIQA/SitePages/LibreChat-guides.aspx?csf=1&amp;web=1&amp;e=evKJpU" target="_blank" rel="noopener noreferrer">LibreChat Worldline guides</a></li><li><a href="https://librechat.worldline-solutions.com/" target="_blank" rel="noopener noreferrer">LibreChat worldline instance</a></li><li><a href="https://www.librechat.ai/" target="_blank" rel="noopener noreferrer">LibreChat official website</a></li><li><a href="https://github.com/danny-avila/LibreChat" target="_blank" rel="noopener noreferrer">LibreChat GitHub repository</a></li><li><a href="https://workspace.google.com/resources/ai/writing-effective-prompts/" target="_blank" rel="noopener noreferrer">Gemini Prompting guide</a></li><li><a href="https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search" target="_blank" rel="noopener noreferrer">Azure search</a></li><li><a href="https://programmablesearchengine.google.com/about/" target="_blank" rel="noopener noreferrer">Google programmable search engine</a></li><li><a href="https://www.anthropic.com/" target="_blank" rel="noopener noreferrer">Claude AI</a></li><li><a href="https://platform.openai.com/docs/assistants/overview" target="_blank" rel="noopener noreferrer">OpenAI Assistant feature</a></li></ul>`,109)])])}const f=a(d,[["render",h]]),g=JSON.parse('{"path":"/3.client/","title":"Online/Offline LLMs clients","lang":"en-US","frontmatter":{"description":"Online/Offline LLMs clients This section is dedicated to the usage of Generative AI models through different clients. We will explore both online and offline clients that allow ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Online/Offline LLMs clients\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2026-02-03T09:35:38.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://worldline.github.io/learning-ai/learning-ai/3.client/"}],["meta",{"property":"og:title","content":"Online/Offline LLMs clients"}],["meta",{"property":"og:description","content":"Online/Offline LLMs clients This section is dedicated to the usage of Generative AI models through different clients. We will explore both online and offline clients that allow ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2026-02-03T09:35:38.000Z"}],["meta",{"property":"article:modified_time","content":"2026-02-03T09:35:38.000Z"}]]},"git":{"updatedTime":1770111338000,"contributors":[{"name":"Brah","username":"Brah","email":"brah.gharbi@gmail.com","commits":10,"url":"https://github.com/Brah"},{"name":"yostane","username":"yostane","email":"1958676+yostane@users.noreply.github.com","commits":1,"url":"https://github.com/yostane"},{"name":"Sylvain Pollet-Villard","username":"","email":"sylvain.pollet.villard@gmail.com","commits":1}],"changelog":[{"hash":"6ca2ee9a5f17d2bdeb1a897f3581ce4ae1a4d797","time":1770111338000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update"},{"hash":"d168b510c3146c98f9dce9c549b5e31cd0f977af","time":1765884845000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"last updates"},{"hash":"0b30e469b0ebd9faf2a0b91f33116c4294ce014c","time":1765876701000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update Librechat client, update RAG for services exercises templates, minor corrections"},{"hash":"1ac1d3c969194d49cf22fae826959aaa0a940758","time":1765546540000,"email":"sylvain.pollet.villard@gmail.com","author":"Sylvain Pollet-Villard","message":"remove legacy endpoint"},{"hash":"1f65cb0a3a8787e0bd042ab23a94eb33ea4aaf98","time":1765467697000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update links"},{"hash":"77688cff7d01fc18c27c4c12356f690ef0c41e58","time":1765460532000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update librechat"},{"hash":"887c7f462510a0a47c61fa89607f525918e8c26a","time":1765358058000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update after review"},{"hash":"2d799747dfdf2759b2ebec625c433d5738895409","time":1751625745000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update structure ased on feedback, added practical work"},{"hash":"5c38433c3ddc1eab892e2f3f2f2311f91fa10475","time":1735037685000,"email":"1958676+yostane@users.noreply.github.com","author":"yostane","message":"update VuePress dependencies and add markdown image plugin"},{"hash":"5d69f8559fe04aab17fa046e9e6e1c301f933164","time":1732110036000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update with prompt techniques"},{"hash":"b0692b48b0492168ddebf087428ca6a41cf3206f","time":1732108523000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update with exercises and Develop content"},{"hash":"341173f4c6f61610db06078d2a1d488986b2e2c5","time":1731692981000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"Update with offline and online prompting"}]},"filePathRelative":"3.client/README.md","autoDesc":true}');export{f as comp,g as data};
