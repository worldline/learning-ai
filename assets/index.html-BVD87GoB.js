import{by as a,bz as n,bA as s,bD as t}from"./app--wUBWlRr.js";const o="/learning-ai/assets/librechat_interface-CYKYbYEb.png",i="/learning-ai/assets/prompt-D9YI1LJh.png",r="/learning-ai/assets/demo_prompt-BpZVtafc.gif",l="/learning-ai/assets/assistant-BJUR0Mbm.png",p="/learning-ai/assets/multi_plugin-TCdQZhy0.png",c="/learning-ai/assets/result_prompt-C4XLLbdh.png",d="/learning-ai/assets/lmstudio-B6PjFtYg.png",h={};function u(m,e){return t(),n("div",null,e[0]||(e[0]=[s('<h1 id="online-offline-llms-clients" tabindex="-1"><a class="header-anchor" href="#online-offline-llms-clients"><span>Online/Offline LLMs clients</span></a></h1><h2 id="online-librechat" tabindex="-1"><a class="header-anchor" href="#online-librechat"><span>Online (Librechat)</span></a></h2><p><img src="'+o+`" alt="librechat_interface"></p><p>LibreChat is a free, open source AI chat platform. This Web UI offers vast customization, supporting numerous AI providers, services, and integrations. Serves all AI Conversations in one place with a familiar interface, innovative enhancements, for as many users as you need.</p><p>The full librechat documentation is available <a href="https://www.librechat.ai/docs" target="_blank" rel="noopener noreferrer">here</a></p><p>Let&#39;s discover how to use LibreChat to create efficient and effective conversations with AI for developers.</p><h3 id="history" tabindex="-1"><a class="header-anchor" href="#history"><span>History</span></a></h3><p>Prompts history allows users to save and load prompts for their conversations and easily access them later. Reusing prompts can save time and effort, especially when working with multiple conversations and keep track of the context and details of a conversation.</p><h3 id="favorites" tabindex="-1"><a class="header-anchor" href="#favorites"><span>Favorites</span></a></h3><p>The favorites feature allows users to save and load favorite prompts for their conversations and easily access them later.</p><h3 id="presets" tabindex="-1"><a class="header-anchor" href="#presets"><span>Presets</span></a></h3><p>The <code>presets</code> feature allows users to save and load predefined settings for initialise a conversations. Users can import and export these presets as JSON files, set a default preset, and share them with others.</p><h3 id="preformatted-prompts" tabindex="-1"><a class="header-anchor" href="#preformatted-prompts"><span>Preformatted prompts</span></a></h3><p>The prompts feature allows users to save and load predefined prompts to use it during their conversations. You can use a prompt with the /[<code>prompt command</code>]. A prompt can have parameters, which are replaced with values when the prompt is used.</p><p><strong>Exemple of preformatted prompts : Explain the following code snippet in Java, Kotlin or Javascript</strong></p><ul><li>Click on the <code>+</code> button to add a new prompt.</li><li>name your prompt : <code>explain</code></li><li>on Text tab, you can write your prompt :</li></ul><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code class="language-text"><span class="line">Explain the following {{language:Java|Kotlin|Javascript}} snippet of code:</span>
<span class="line">{{code}}</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="`+i+'" alt="preformatted_prompts_config"></p><ul><li>Now you can use the <code>/explain</code> command to get the explanation of the code snippet.</li></ul><p><img src="'+r+'" alt="preformatted_prompts_usage"></p><h3 id="ai-providers" tabindex="-1"><a class="header-anchor" href="#ai-providers"><span>AI providers</span></a></h3><h4 id="azure-openai" tabindex="-1"><a class="header-anchor" href="#azure-openai"><span>Azure OpenAI</span></a></h4><p>Azure OpenAI Service provides REST API access to OpenAI&#39;s powerful language models, including the o1-preview, o1-mini, GPT-4o, GPT-4o mini, GPT-4 Turbo with Vision, GPT-4, GPT-3.5-Turbo, and Embeddings model series.</p><h4 id="google-gemini" tabindex="-1"><a class="header-anchor" href="#google-gemini"><span>Google Gemini</span></a></h4><p>Gemini is a large language model (LLM) developed by Google. It&#39;s designed to be a multimodal AI, meaning it can work with and understand different types of information, including text, code, audio, and images. Google positions Gemini as a highly capable model for a range of tasks, from answering questions and generating creative content to problem-solving and more complex reasoning. There are different versions of Gemini, optimized for different tasks and scales.</p><h4 id="anthropic-claude" tabindex="-1"><a class="header-anchor" href="#anthropic-claude"><span>Anthropic Claude</span></a></h4><p>Claude is an Artificial Intelligence, trained by Anthropic. Claude can process large amounts of information, brainstorm ideas, generate text and code, help you understand subjects, coach you through difficult situations, help simplify your busywork so you can focus on what matters most, and so much more.</p><h3 id="assistants" tabindex="-1"><a class="header-anchor" href="#assistants"><span>Assistants</span></a></h3><p>The Assistants API enables the creation of AI assistants, offering functionalities like code interpreter, knowledge retrieval of files, and function execution. The Assistants API allows you to build AI assistants within your own applications for specific needs. An Assistant has instructions and can leverage models, tools, and files to respond to user queries. The Assistants API currently supports three types of tools: Code Interpreter, File Search, and Function calling.</p><p><img src="'+l+`" alt="assistant"></p><h3 id="plugins" tabindex="-1"><a class="header-anchor" href="#plugins"><span>Plugins</span></a></h3><p>The plugins endpoint opens the door to prompting LLMs in new ways other than traditional input/output prompting.</p><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>Every additional plugin selected will increase your token usage as there are detailed instructions the LLM needs for each one For best use, be selective with plugins per message and narrow your requests as much as possible</p></div><h4 id="dall-e-3" tabindex="-1"><a class="header-anchor" href="#dall-e-3"><span>DALL-E 3</span></a></h4><p>Dall-e 3 is a librechat Plugin for generating images from text. You can use it to generate images from text, such as product descriptions, product images, or even documentation images to illustrate your technical documentation.</p><h4 id="confluence" tabindex="-1"><a class="header-anchor" href="#confluence"><span>Confluence</span></a></h4><p>Ask confluence is a librechat Plugin for Confluence documents.</p><h4 id="it-support" tabindex="-1"><a class="header-anchor" href="#it-support"><span>IT support</span></a></h4><p>Ask for IT support enable you to get support from the IT team and create WLSD tickets from your chats.</p><h4 id="wolf" tabindex="-1"><a class="header-anchor" href="#wolf"><span>WOLF</span></a></h4><p><code>Wolf</code> is a librechat Plugin for WL Managagement System documents. The sharepoint documention is available <a href="https://worldline365.sharepoint.com/sites/AAC815" target="_blank" rel="noopener noreferrer">here</a></p><p>Ask to WorldLine management system Friend everything you are looking for in the WMS content. AskWOLF plugin is meant to help you navigate through the multitude of information provided by the WMS (Applicable Policies, Processes &amp; Procedures, Transversal &amp; Operations SP pages links, …). This Worldline LibreChat plugin relies on ChatGPT technologies.</p><p>​​​​​​​Worldline Management System (WMS) is the Group reference for all information pertaining to our operating model such as applicable policies, processes and governance structures. Key responsibilities are :</p><ul><li>consistently address its customers’ and markets’ requirements across all its geographies</li><li>continuous improvement of customer satisfaction through effective application of WMS</li><li>correct interpretation of applicable ISO standards requirements</li></ul><p>Example of prompts:</p><ul><li>AskWOLF: What is the WMS?</li><li>AskWOLF: What are the policies?</li><li>AskWOLF: What are the processes?</li></ul><h4 id="browse-plugins" tabindex="-1"><a class="header-anchor" href="#browse-plugins"><span>Browse plugins</span></a></h4><p>Retrieve data from internet and use it to generate a response.</p><h3 id="plugin-mixing" tabindex="-1"><a class="header-anchor" href="#plugin-mixing"><span>Plugin mixing</span></a></h3><p>You can mix plugins to create more complex prompts. For example, you can use the DALL-E 3 plugin to generate images from text and then use the IT support plugin to get support from the IT team.</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code class="language-text"><span class="line">Generate the favicon 16x16 pixels based on the content found in</span>
<span class="line">https://worldline.github.io/learning-ai/overview/ with Browser plugin</span>
<span class="line">and generate the favicon with DallE. I want no background and black and white image</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="`+p+'" alt="prompt"><img src="'+c+'" alt="Favicon"></p><h3 id="rag" tabindex="-1"><a class="header-anchor" href="#rag"><span>RAG</span></a></h3><p>RAG is possible with LibreChat. You can use RAG to create a conversation with the AI. To can add files to the conversation, you go to the file tab and select the file you want to add. Then the file will be added to the file manager and you can use it in the prompt.</p><p>The file can be an png, a video, a text file, or a PDF file.</p><h3 id="🧪-exercises" tabindex="-1"><a class="header-anchor" href="#🧪-exercises"><span>🧪 Exercises</span></a></h3><h4 id="_1-prompt-creation" tabindex="-1"><a class="header-anchor" href="#_1-prompt-creation"><span>1. Prompt creation</span></a></h4><p>Select one prompt engineering technique and make a prompt in librechat that can be called with the <code>/[prompt_name]</code> command.</p><h4 id="_2-plugins-mixing" tabindex="-1"><a class="header-anchor" href="#_2-plugins-mixing"><span>2. Plugins mixing</span></a></h4><p>Use the Browse and Dall-E plugins to create a prompt that generates a a favicon based on the content of this learning-ai website.</p><h4 id="_3-make-your-own-assistant" tabindex="-1"><a class="header-anchor" href="#_3-make-your-own-assistant"><span>3. Make your own assistant</span></a></h4><p>Choose your favorite topic ( cooking, travel, sports, etc.) and create an assistant that can answer questions about it. You can share documents, files and instructions to configure your custom assistant and use it.</p><h2 id="offline-lm-studio" tabindex="-1"><a class="header-anchor" href="#offline-lm-studio"><span>Offline (LM Studio)</span></a></h2><div class="hint-container warning"><p class="hint-container-title">Disclamer</p><p>Be careful with offline prompting models downloaded from the internet. They can contain malicious code. And also the size of the model can be very large from few Gb to few Tb.</p></div><h3 id="definitions" tabindex="-1"><a class="header-anchor" href="#definitions"><span>Definitions</span></a></h3><p>If you don&#39;t want to use the online AI providers, you can use offline prompting. This technique involves using a local LLM to generate responses to prompts. It is useful for developers who want to use a local LLM for offline prompting or for those who want to experiment with different LLMs without relying on online providers.</p><p>LM Studio is a tool that allows developers to experiment with different LLMs without relying on online providers. It provides a user-friendly interface for selecting and configuring LLMs, as well as a chat interface for interacting with the LLMs. It also includes features for fine-tuning and deploying LLMs. This technique is useful for developers who want to experiment with different LLMs.</p><h3 id="installation" tabindex="-1"><a class="header-anchor" href="#installation"><span>Installation</span></a></h3><p><img src="'+d+`" alt="lmstudio_installation"></p><p>For installation, you can follow the instructions <a href="https://lmstudio.ai/docs/" target="_blank" rel="noopener noreferrer">here</a></p><h3 id="model-configuration" tabindex="-1"><a class="header-anchor" href="#model-configuration"><span>Model configuration</span></a></h3><p>You can configure the model you want to use in the settings tab. You can select the model you want to use and configure it according to your needs.</p><p><code>Context Length</code>: The context length is the number of tokens that will be used as context for the model. This is important because it determines how much information the model can use to generate a response. A longer context length will allow the model to generate more detailed and relevant responses, but it may also increase the computational cost of the model.</p><p><code>GPU Offload</code>: This option allows you to offload the model to a GPU if available. This can significantly speed up the generation process, especially for longer prompts or complex models.</p><p><code>CPU Threads</code>: This option allows you to specify the number of CPU threads to use for the model. This can be useful for controlling the computational resources used by the model.</p><p><code>Evaluation batch size</code>: This option allows you to specify the batch size for evaluation. This is important for evaluating the performance of the model and can affect the speed and accuracy of the generation process.</p><p><a href="https://www.hopsworks.ai/dictionary/rope-scaling" target="_blank" rel="noopener noreferrer"><code>RoPE Frequency base</code></a>: The Rotary Position Embeddings is a technique used to improve the performance of transformer-based models. This is important for controlling the output length of the model and can affect the quality of the generated responses.</p><p><code>Keep model in memory</code>: This option allows you to keep the model in memory after the generation process is complete. This can be useful for generating multiple responses or for using the model for offline prompting.</p><p><code>Try mmap()</code> for faster loading: This option allows you to try using mmap() for faster loading of the model. This can be useful for loading large models or for generating responses quickly.</p><p><code>Seed</code>: This option allows you to specify a seed for the model. This can be useful for controlling the randomness of the generated responses.</p><p><code>Flash Attention</code>: This option allows you to enable flash attention for the model. This can be useful for generating more detailed and accurate responses, but it may also increase the computational cost of the model.</p><h3 id="enable-apis" tabindex="-1"><a class="header-anchor" href="#enable-apis"><span>enable APIs</span></a></h3><p>You can use the APIs to generate responses from the models. To enable the API server with LM Studio, you need to set the <code>API Server</code> option to <code>ON</code> in the settings tab. You can then use the API endpoints to generate responses from the models.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Success<span class="token operator">!</span> HTTP server listening on port <span class="token number">1234</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Supported endpoints:</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	GET  http://localhost:1234/v1/models</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/chat/completions</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/completions</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/embeddings</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Logs are saved into /Users/ibrahim/.cache/lm-studio/server-logs</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> Server started.</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> Just-in-time model loading active.</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You can use the endpoints to generate responses from the models. The endpoints are as follows:</p><ul><li><code>GET /v1/models</code>: This endpoint returns a list of the available models.</li><li><code>POST /v1/chat/completions</code>: This endpoint generates responses from the models using the chat format.Chat format is used for tasks such as chatbots, conversational AI, and language learning.</li><li><code>POST /v1/completions</code>: This endpoint generates responses from the models using the completion format. Completion format is used for tasks such as question answering, summarization, and text generation.</li><li><code>POST /v1/embeddings</code>: This endpoint generates embeddings from the models. Embeddings are used for tasks such as sentiment analysis, text classification, and language translation.</li></ul><h3 id="🧪-exercises-1" tabindex="-1"><a class="header-anchor" href="#🧪-exercises-1"><span>🧪 Exercises</span></a></h3><ol><li>Install LM Studio and configure the model you want to use.</li><li>Enable the API server and test the endpoints using curl or Postman.</li></ol><h2 id="📖-further-readings" tabindex="-1"><a class="header-anchor" href="#📖-further-readings"><span>📖 Further readings</span></a></h2><ul><li><a href="https://worldline365.sharepoint.com/:u:/r/sites/GenerativeAIQA/SitePages/LibreChat-guides.aspx?csf=1&amp;web=1&amp;e=evKJpU" target="_blank" rel="noopener noreferrer">LibreChat Worldline guides</a></li><li><a href="https://librechat.as8677.net/login" target="_blank" rel="noopener noreferrer">LibreChat worldline instance</a></li><li><a href="https://www.librechat.ai/" target="_blank" rel="noopener noreferrer">LibreChat official website</a></li><li><a href="https://github.com/danny-avila/LibreChat" target="_blank" rel="noopener noreferrer">LibreChat GitHub repository</a></li><li><a href="">Gemini Prompting guide</a></li><li><a href="https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search" target="_blank" rel="noopener noreferrer">Azure search</a></li><li><a href="https://programmablesearchengine.google.com/about/" target="_blank" rel="noopener noreferrer">Google programmable search engine</a></li><li><a href="https://www.anthropic.com/" target="_blank" rel="noopener noreferrer">Claude AI</a></li><li><a href="https://platform.openai.com/docs/assistants/overview" target="_blank" rel="noopener noreferrer">OpenAI Assistant feature</a></li></ul>`,90)]))}const g=a(h,[["render",u]]),b=JSON.parse('{"path":"/3.client/","title":"Online/Offline LLMs clients","lang":"en-US","frontmatter":{"description":"Online/Offline LLMs clients Online (Librechat) librechat_interface LibreChat is a free, open source AI chat platform. This Web UI offers vast customization, supporting numerous ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Online/Offline LLMs clients\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-07-04T10:42:25.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://worldline.github.io/learning-ai/learning-ai/3.client/"}],["meta",{"property":"og:title","content":"Online/Offline LLMs clients"}],["meta",{"property":"og:description","content":"Online/Offline LLMs clients Online (Librechat) librechat_interface LibreChat is a free, open source AI chat platform. This Web UI offers vast customization, supporting numerous ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-07-04T10:42:25.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-04T10:42:25.000Z"}]]},"git":{"updatedTime":1751625745000,"contributors":[{"name":"Brah","username":"Brah","email":"brah.gharbi@gmail.com","commits":4,"url":"https://github.com/Brah"},{"name":"yostane","username":"yostane","email":"1958676+yostane@users.noreply.github.com","commits":1,"url":"https://github.com/yostane"}],"changelog":[{"hash":"2d799747dfdf2759b2ebec625c433d5738895409","time":1751625745000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update structure ased on feedback, added practical work"},{"hash":"5c38433c3ddc1eab892e2f3f2f2311f91fa10475","time":1735037685000,"email":"1958676+yostane@users.noreply.github.com","author":"yostane","message":"update VuePress dependencies and add markdown image plugin"},{"hash":"5d69f8559fe04aab17fa046e9e6e1c301f933164","time":1732110036000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update with prompt techniques"},{"hash":"b0692b48b0492168ddebf087428ca6a41cf3206f","time":1732108523000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"update with exercises and Develop content"},{"hash":"341173f4c6f61610db06078d2a1d488986b2e2c5","time":1731692981000,"email":"brah.gharbi@gmail.com","author":"Brah","message":"Update with offline and online prompting"}]},"filePathRelative":"3.client/README.md","autoDesc":true}');export{g as comp,b as data};
